Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Zhang2019,
abstract = {Text classification tends to be difficult when the data is deficient or when it is required to adapt to unseen classes. In such challenging scenarios, recent studies have often used meta-learning to simulate the few-shot task, thus negating explicit common linguistic features across tasks. Deep language representations have proven to be very effective forms of unsupervised pretraining, yielding contextual-ized features that capture linguistic properties and benefit downstream natural language understanding tasks. However, the effect of pretrained language representation for few-shot learning on text classification tasks is still not well understood. In this study, we design a few-shot learning model with pretrained language representations and report the empirical results. We show that our approach is not only simple but also produces state-of-the-art performance on a well-studied sentiment classification dataset. It can thus be further suggested that pretraining could be a promising solution for few shot learning of many other NLP tasks. The code and the dataset to replicate the experiments are made available at https://github.com/zxlzr/FewShotNLP.},
archivePrefix = {arXiv},
arxivId = {1908.08788v1},
author = {Zhang, Ningyu and Sun, Zhanlin and Deng, Shumin and Chen, Jiaoyan and Chen, Huajun},
eprint = {1908.08788v1},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Zhang et al. - 2019 - Improving Few-shot Text Classification via Pretrained Language Representations.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
title = {{Improving Few-shot Text Classification via Pretrained Language Representations}},
url = {https://github.com/Gorov/DiverseFewShot},
year = {2019}
}
@techreport{Wolf,
author = {Wolf, Thomas and Chaumond, Julien and Delangue, Clement},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Wolf, Chaumond, Delangue - Unknown - Workshop track-ICLR 2018 META-LEARNING A DYNAMICAL LANGUAGE MODEL.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
title = {{Workshop track-ICLR 2018 META-LEARNING A DYNAMICAL LANGUAGE MODEL}},
url = {https://openreview.net/pdf?id=SyikZmkDM}
}
@article{Chen2018,
abstract = {Semantic composition functions have been playing a pivotal role in neural representation learning of text sequences. In spite of their success, most existing models suffer from the underfitting problem: they use the same shared compositional function on all the positions in the sequence, thereby lacking expressive power due to incapacity to capture the richness of compositionality. Besides, the composition functions of different tasks are independent and learned from scratch. In this paper, we propose a new sharing scheme of composition function across multiple tasks. Specifically, we use a shared meta-network to capture the meta-knowledge of semantic composition and generate the parameters of the task-specific semantic composition models. We conduct extensive experiments on two types of tasks, text classification and sequence tagging, which demonstrate the benefits of our approach. Besides, we show that the shared meta-knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.},
author = {Chen, Junkun and Qiu, Xipeng and Liu, Pengfei and Huang, Xuanjing},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/17140-77564-1-PB.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
keywords = {Natural Language Processing and Machine Learning T,meta},
mendeley-tags = {meta},
pages = {5070--5077},
title = {{Meta multi-task learning for sequence modeling}},
year = {2018}
}
@techreport{Gu,
abstract = {In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen Euro-pean languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outper-forms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT'16 by seeing only 16,000 translated words (â‡  600 parallel sentences).},
author = {Gu, Jiatao and Wang, Yong and Chen, Yun and Cho, Kyunghyun and Li, Victor OK},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Gu et al. - Unknown - Meta-Learning for Low-Resource Neural Machine Translation.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
pages = {3622--3631},
title = {{Meta-Learning for Low-Resource Neural Machine Translation}},
url = {https://aclweb.org/anthology/D18-1398}
}
@techreport{Yu,
abstract = {We study few-shot learning in natural language domains. Compared to many existing works that apply either metric-based or optimization-based meta-learning to image domain with low inter-task variance, we consider a more realistic setting, where tasks are diverse. However, it imposes tremendous difficulties to existing state-of-the-art metric-based algorithms since a single metric is insufficient to capture complex task variations in natural language domain. To alleviate the problem , we propose an adaptive metric learning approach that automatically determines the best weighted combination from a set of met-rics obtained from meta-training tasks for a newly seen few-shot task. Extensive quantitative evaluations on real-world sentiment analysis and dialog intent classification datasets demonstrate that the proposed method performs favorably against state-of-the-art few shot learning algorithms in terms of predictive accuracy. We make our code and data available for further study. 1},
author = {Yu, Mo and Guo, Xiaoxiao and Yi, Jinfeng and {Chang Saloni Potdar Yu Cheng Gerald Tesauro Haoyu Wang Bowen Zhou}, Shiyu},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Yu et al. - Unknown - Diverse Few-Shot Text Classification with Multiple Metrics.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
pages = {1206--1215},
title = {{Diverse Few-Shot Text Classification with Multiple Metrics}},
url = {https://github.com/Gorov/DiverseFewShot{\_}Amazon}
}
@article{Finn2017,
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
archivePrefix = {arXiv},
arxivId = {1703.03400},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
eprint = {1703.03400},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Finn, Abbeel, Levine - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf:pdf},
keywords = {meta},
month = {mar},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {http://arxiv.org/abs/1703.03400},
year = {2017}
}
@techreport{Nichol,
abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
archivePrefix = {arXiv},
arxivId = {1803.02999v3},
author = {Nichol, Alex and Achiam, Joshua and Openai, John Schulman},
eprint = {1803.02999v3},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Nichol, Achiam, Openai - Unknown - On First-Order Meta-Learning Algorithms.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
title = {{On First-Order Meta-Learning Algorithms}},
url = {https://arxiv.org/pdf/1803.02999.pdf}
}
