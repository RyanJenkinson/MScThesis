Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Ruder,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747v2},
author = {Ruder, Sebastian},
eprint = {1609.04747v2},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Ruder - Unknown - An overview of gradient descent optimization algorithms.pdf:pdf},
title = {{An overview of gradient descent optimization algorithms *}},
url = {http://caffe.berkeleyvision.org/tutorial/solver.html}
}
@techreport{Brown,
abstract = {We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.},
annote = {n gram reference},
author = {Brown, Peter F and DeSouza, Peter V and Mercer, Robert L and {Della Pietra}, Vincent J and Lai, Jenifer C},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Brown et al. - Unknown - Class-Based n-gram Models of Natural Language.pdf:pdf},
title = {{Class-Based n-gram Models of Natural Language}},
url = {https://www.aclweb.org/anthology/J92-4003}
}
@article{Huang2016,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
eprint = {1608.06993},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:pdf},
month = {aug},
title = {{Densely Connected Convolutional Networks}},
url = {http://arxiv.org/abs/1608.06993},
year = {2016}
}
@article{Howard2018,
abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24{\%} on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
archivePrefix = {arXiv},
arxivId = {1801.06146},
author = {Howard, Jeremy and Ruder, Sebastian},
eprint = {1801.06146},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Howard, Ruder - 2018 - Universal Language Model Fine-tuning for Text Classification.pdf:pdf},
month = {jan},
title = {{Universal Language Model Fine-tuning for Text Classification}},
url = {http://arxiv.org/abs/1801.06146},
year = {2018}
}
@techreport{Wolf,
author = {Wolf, Thomas and Chaumond, Julien and Delangue, Clement},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Wolf, Chaumond, Delangue - Unknown - Workshop track-ICLR 2018 META-LEARNING A DYNAMICAL LANGUAGE MODEL.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
title = {{Workshop track-ICLR 2018 META-LEARNING A DYNAMICAL LANGUAGE MODEL}},
url = {https://openreview.net/pdf?id=SyikZmkDM}
}
@article{Parisi2018,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
archivePrefix = {arXiv},
arxivId = {1802.07569},
author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
doi = {10.1016/j.neunet.2019.01.012.},
eprint = {1802.07569},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Parisi et al. - 2018 - Continual Lifelong Learning with Neural Networks A Review.pdf:pdf},
keywords = {multitask},
mendeley-tags = {multitask},
month = {feb},
title = {{Continual Lifelong Learning with Neural Networks: A Review}},
url = {http://arxiv.org/abs/1802.07569 http://dx.doi.org/10.1016/j.neunet.2019.01.012.},
year = {2018}
}
@techreport{Hochreiter1997,
abstract = {Learning to store information over extended time intervals via recurrent backpropagation takes a very long time, mostly due to insuucient, decaying error back ow. We brieey review Hochreiter's 1991 analysis of this problem, then address it by introducing a novel, eecient, gradient-based method called $\backslash$Long Short-Term Memory" (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete time steps by enforcing constant error ow through $\backslash$constant error carrousels" within special units. Multiplicative gate units learn to open and close access to the constant error ow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artiicial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with RTRL, BPTT, Recurrent Cascade-Correlation, Elman nets, and Neural Sequence Chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artiicial long time lag tasks that have never been solved by previous recurrent network algorithms.},
author = {Hochreiter, Sepp and {Urgen Schmidhuber}, J J},
booktitle = {MEMORY Neural Computation},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Hochreiter, Urgen Schmidhuber - 1997 - Long Short-Term Memory.pdf:pdf},
number = {8},
pages = {1735--1780},
title = {{Long Short-Term Memory}},
url = {http://www7.informatik.tu-muenchen.de/{~}hochreithttp://www.idsia.ch/{~}juergen},
volume = {9},
year = {1997}
}
@misc{XLNetTeam2019,
author = {{XLNet Team}},
booktitle = {Medium Article},
title = {{A Fair Comparison Study of XLNet and BERT with Large Models}},
url = {https://medium.com/@xlnet.team/a-fair-comparison-study-of-xlnet-and-bert-with-large-models-5a4257f59dc0},
urldate = {2019-08-19},
year = {2019}
}
@article{Peters2018a,
abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
archivePrefix = {arXiv},
arxivId = {1802.05365},
author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
eprint = {1802.05365},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers//Peters et al. - 2018 - Deep contextualized word representations.pdf:pdf},
month = {feb},
title = {{Deep contextualized word representations}},
url = {http://arxiv.org/abs/1802.05365},
year = {2018}
}
@article{Mikolov2013,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
annote = {skip gram reference},
archivePrefix = {arXiv},
arxivId = {1301.3781},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
month = {jan},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/abs/1301.3781},
year = {2013}
}
@article{Ruder2017,
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
eprint = {1706.05098},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural Networks.pdf:pdf},
keywords = {multitask},
month = {jun},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
url = {http://arxiv.org/abs/1706.05098},
year = {2017}
}
@article{Kaur2014,
author = {Kaur, Gaganpreet},
doi = {10.15623/ijret.2014.0301026},
journal = {International Journal of Research in Engineering and Technology},
month = {jan},
number = {01},
pages = {168--174},
title = {{USAGE OF REGULAR EXPRESSIONS IN NLP}},
url = {https://ijret.org/volumes/2014v03/i01/IJRET20140301026.pdf},
volume = {03},
year = {2014}
}
@techreport{RadfordGPT,
abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9{\%} on commonsense reasoning (Stories Cloze Test), 5.7{\%} on question answering (RACE), and 1.5{\%} on textual entailment (MultiNLI).},
author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Openai et al. - Unknown - Improving Language Understanding by Generative Pre-Training.pdf:pdf},
title = {{Improving Language Understanding by Generative Pre-Training}},
url = {https://gluebenchmark.com/leaderboard}
}
@article{Sun2019a,
abstract = {Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
archivePrefix = {arXiv},
arxivId = {1907.12412},
author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
eprint = {1907.12412},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Sun et al. - 2019 - ERNIE 2.0 A Continual Pre-training Framework for Language Understanding.pdf:pdf},
keywords = {multitask},
mendeley-tags = {multitask},
month = {jul},
title = {{ERNIE 2.0: A Continual Pre-training Framework for Language Understanding}},
url = {http://arxiv.org/abs/1907.12412},
year = {2019}
}
@techreport{Socher,
abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model out-performs all previous methods on several met-rics. It pushes the state of the art in single sentence positive/negative classification from 80{\%} up to 85.4{\%}. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7{\%}, an improvement of 9.7{\%} over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases.},
annote = {SST-2},
author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Socher et al. - Unknown - Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank.pdf:pdf},
title = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
url = {http://nlp.stanford.edu/}
}
@techreport{Tjong,
abstract = {We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance .},
author = {Tjong, Erik F and Sang, Kim and {De Meulder}, Fien},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Tjong, Sang, De Meulder - Unknown - Introduction to the CoNLL-2003 Shared Task Language-Independent Named Entity Recognition.pdf:pdf},
title = {{Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition}},
url = {http://lcg-www.uia.ac.be/conll2003/ner/}
}
@article{Sanh2018,
abstract = {Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.},
annote = {https://medium.com/dair-ai/hmtl-multi-task-learning-for-state-of-the-art-nlp-245572bbb601},
archivePrefix = {arXiv},
arxivId = {1811.06031},
author = {Sanh, Victor and Wolf, Thomas and Ruder, Sebastian},
eprint = {1811.06031},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Sanh, Wolf, Ruder - 2018 - A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks.pdf:pdf},
keywords = {multitask},
mendeley-tags = {multitask},
month = {nov},
title = {{A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks}},
url = {http://arxiv.org/abs/1811.06031},
year = {2018}
}
@article{Yang2019,
abstract = {With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking.},
archivePrefix = {arXiv},
arxivId = {1906.08237},
author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
eprint = {1906.08237},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Yang et al. - 2019 - XLNet Generalized Autoregressive Pretraining for Language Understanding.pdf:pdf},
month = {jun},
title = {{XLNet: Generalized Autoregressive Pretraining for Language Understanding}},
url = {http://arxiv.org/abs/1906.08237},
year = {2019}
}
@techreport{Szegedy,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error and 17.3{\%} top-1 error.},
annote = {Label Smoothing},
archivePrefix = {arXiv},
arxivId = {1512.00567v3},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon},
eprint = {1512.00567v3},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Szegedy et al. - Unknown - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {1512.00567v3},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {https://arxiv.org/pdf/1512.00567.pdf}
}
@techreport{VanDerMaaten2008,
abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
booktitle = {Journal of Machine Learning Research},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Van Der Maaten, Hinton - 2008 - Visualizing Data using t-SNE.pdf:pdf},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
title = {{Visualizing Data using t-SNE}},
url = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
volume = {9},
year = {2008}
}
@article{Radford2017,
abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
archivePrefix = {arXiv},
arxivId = {1704.01444},
author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
eprint = {1704.01444},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Radford, Jozefowicz, Sutskever - 2017 - Learning to Generate Reviews and Discovering Sentiment.pdf:pdf},
month = {apr},
title = {{Learning to Generate Reviews and Discovering Sentiment}},
url = {http://arxiv.org/abs/1704.01444},
year = {2017}
}
@techreport{Nilsson,
annote = {For the history of AI, repopularisation of DL etc},
author = {Nilsson, Nils J},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Nilsson - Unknown - THE QUEST FOR ARTIFICIAL INTELLIGENCE A HISTORY OF IDEAS AND ACHIEVEMENTS.pdf:pdf},
isbn = {0521122937},
title = {{THE QUEST FOR ARTIFICIAL INTELLIGENCE A HISTORY OF IDEAS AND ACHIEVEMENTS}},
url = {http://www.cambridge.org/us/0521122937http://www.cambridge.org/us/0521122937http://www.cambridge.org/us/0521122937}
}
@article{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
annote = {Resnet reference},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Baxter1997,
author = {Baxter, Jonathan},
doi = {10.1023/A:1007327622663},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Baxter - 1997 - A BayesianInformation Theoretic Model of Learning to Learn via Multiple Task Sampling.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {multitask},
number = {1},
pages = {7--39},
publisher = {Kluwer Academic Publishers},
title = {{A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling}},
url = {http://link.springer.com/10.1023/A:1007327622663},
volume = {28},
year = {1997}
}
@techreport{Mikolov,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
annote = {Builds upon the skip gram model},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Mikolov et al. - Unknown - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
url = {https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}
@techreport{Parikh,
abstract = {We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subprob-lems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.},
author = {Parikh, Ankur and Tackstrom, Oscar and Dipanjan, Das and Uszkoreit, Jakob},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Parikh et al. - Unknown - A Decomposable Attention Model for Natural Language Inference.pdf:pdf},
title = {{A Decomposable Attention Model for Natural Language Inference}},
url = {https://aclweb.org/anthology/D16-1244}
}
@techreport{Werbos,
abstract = {Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification , and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives the theorem which underlies backpropagation-is briefly discussed.},
author = {Werbos, Paul J},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Werbos - Unknown - Backpropagation Through Time What It Does and How to Do It.pdf:pdf},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
url = {http://www.cs.cmu.edu/{~}bhiksha/courses/deeplearning/Fall.2016/pdfs/Werbos.backprop.pdf}
}
@techreport{Pascanu,
abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1211.5063v2},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
eprint = {1211.5063v2},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Pascanu, Mikolov, Bengio - Unknown - On the difficulty of training Recurrent Neural Networks.pdf:pdf},
title = {{On the difficulty of training Recurrent Neural Networks}},
url = {https://arxiv.org/pdf/1211.5063.pdf}
}
@article{Chen2018,
abstract = {Semantic composition functions have been playing a pivotal role in neural representation learning of text sequences. In spite of their success, most existing models suffer from the underfitting problem: they use the same shared compositional function on all the positions in the sequence, thereby lacking expressive power due to incapacity to capture the richness of compositionality. Besides, the composition functions of different tasks are independent and learned from scratch. In this paper, we propose a new sharing scheme of composition function across multiple tasks. Specifically, we use a shared meta-network to capture the meta-knowledge of semantic composition and generate the parameters of the task-specific semantic composition models. We conduct extensive experiments on two types of tasks, text classification and sequence tagging, which demonstrate the benefits of our approach. Besides, we show that the shared meta-knowledge learned by our proposed model can be regarded as off-the-shelf knowledge and easily transferred to new tasks.},
author = {Chen, Junkun and Qiu, Xipeng and Liu, Pengfei and Huang, Xuanjing},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/17140-77564-1-PB.pdf:pdf},
isbn = {9781577358008},
journal = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
keywords = {Natural Language Processing and Machine Learning T,meta},
mendeley-tags = {meta},
pages = {5070--5077},
title = {{Meta multi-task learning for sequence modeling}},
year = {2018}
}
@article{Cheng2016,
abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
archivePrefix = {arXiv},
arxivId = {1601.06733},
author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
eprint = {1601.06733},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Cheng, Dong, Lapata - 2016 - Long Short-Term Memory-Networks for Machine Reading.pdf:pdf},
month = {jan},
title = {{Long Short-Term Memory-Networks for Machine Reading}},
url = {http://arxiv.org/abs/1601.06733},
year = {2016}
}
@article{Devlin2018,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
eprint = {1810.04805},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:pdf},
month = {oct},
title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
url = {http://arxiv.org/abs/1810.04805},
year = {2018}
}
@techreport{Cai2017,
abstract = {Graph is an important data representation which appears in a wide diversity of real-world scenarios. Effective graph analytics provides users a deeper understanding of what is behind the data, and thus can benefit a lot of useful applications such as node classification, node recommendation, link prediction, etc. However, most graph analytics methods suffer the high computation and space cost. Graph embedding is an effective yet efficient way to solve the graph analytics problem. It converts the graph data into a low dimensional space in which the graph structural information and graph properties are maximumly preserved. In this survey, we conduct a comprehensive review of the literature in graph embedding. We first introduce the formal definition of graph embedding as well as the related concepts. After that, we propose two taxonomies of graph embedding which correspond to what challenges exist in different graph embedding problem settings and how the existing work address these challenges in their solutions. Finally, we summarize the applications that graph embedding enables and suggest four promising future research directions in terms of computation efficiency, problem settings, techniques and application scenarios.},
archivePrefix = {arXiv},
arxivId = {1709.07604v3},
author = {Cai, Hongyun and Zheng, Vincent W and {Chen-Chuan Chang}, Kevin},
booktitle = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
eprint = {1709.07604v3},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Cai, Zheng, Chen-Chuan Chang - 2017 - A Comprehensive Survey of Graph Embedding Problems, Techniques and Applications.pdf:pdf},
keywords = {Index Terms-Graph embedding,graph analytics,graph embedding survey,network embedding !},
pages = {1},
title = {{A Comprehensive Survey of Graph Embedding: Problems, Techniques and Applications}},
url = {https://arxiv.org/pdf/1709.07604.pdf},
volume = {XX},
year = {2017}
}
@article{Varsamopoulos2018,
abstract = {Recent works have shown that small distance quantum error correction codes can be efficiently decoded by employing machine learning techniques like neural networks. Various techniques employing neural networks have been used to increase the decoding performance, however, there is no framework that analyses a step-by-step procedure in designing such a neural network based decoder. The main objective of this work is to present a detailed framework that investigates the way that various neural network parameters affect the decoding performance, the training time and the execution time of a neural network based decoder. We focus on neural network parameters such as the size of the training dataset, the structure, and the type of the neural network, the batch size and the learning rate used during training. We compare various decoding strategies and showcase how these influence the objectives for different error models. For the noiseless syndrome measurements, we reach up to 50$\backslash${\%} improvement against the benchmark algorithm (Blossom) and for the noisy syndrome measurements, we show efficient decoding performance up to 97 qubits for the rotated Surface code. Furthermore, we show that the scaling of the execution time (time calculated during the inference phase) is linear with the number of qubits, which is a significant improvement compared to existing classical decoding algorithms that scale polynomially with the number of qubits.},
annote = {Used for LSTM image credits},
archivePrefix = {arXiv},
arxivId = {1811.12456},
author = {Varsamopoulos, Savvas and Bertels, Koen and Almudever, Carmen G.},
eprint = {1811.12456},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Varsamopoulos, Bertels, Almudever - 2018 - Designing neural network based decoders for surface codes.pdf:pdf},
month = {nov},
title = {{Designing neural network based decoders for surface codes}},
url = {http://arxiv.org/abs/1811.12456},
year = {2018}
}
@techreport{Nederhof,
abstract = {We compare the asymptotic time complexity of left-to-right and bidirectional parsing techniques for bilexical context-free grammars, a grammar formalism that is an abstraction of language models used in several state-of-the-art real-world parsers. We provide evidence that left-to-right parsing cannot be re-alised within acceptable time-bounds if the so called correct-prefix property is to be ensured. Our evidence is based on complexity results for the representation of regular languages.},
author = {Nederhof, Mark-Jan and Stuhlsatzenhausweg, Dfki and Germany, Saarbrficken and Satta, Giorgio},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Nederhof et al. - Unknown - Left-To-Right Parsing and Bilexical Context-Free Grammars.pdf:pdf},
title = {{Left-To-Right Parsing and Bilexical Context-Free Grammars}},
url = {https://www.aclweb.org/anthology/A00-2036}
}
@article{Ba2016,
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
archivePrefix = {arXiv},
arxivId = {1607.06450},
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
eprint = {1607.06450},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Ba, Kiros, Hinton - 2016 - Layer Normalization.pdf:pdf},
month = {jul},
title = {{Layer Normalization}},
url = {http://arxiv.org/abs/1607.06450},
year = {2016}
}
@techreport{Pennington,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log-bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co-occurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful sub-structure, as evidenced by its performance of 75{\%} on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Pennington, Socher, Manning - Unknown - GloVe Global Vectors for Word Representation.pdf:pdf},
title = {{GloVe: Global Vectors for Word Representation}},
url = {https://nlp.stanford.edu/projects/glove/}
}
@article{Harris1954,
author = {Harris, Zellig S},
doi = {10.1080/00437956.1954.11659520},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Harris - 1954 - Distributional Structure.pdf:pdf},
journal = {Distributional Structure, WORD},
number = {3},
pages = {146--162},
title = {{Distributional Structure}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=rwrd20},
volume = {10},
year = {1954}
}
@techreport{Jain,
abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful "explanations" for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.},
archivePrefix = {arXiv},
arxivId = {1902.10186v3},
author = {Jain, Sarthak and Wallace, Byron C},
eprint = {1902.10186v3},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Jain, Wallace - Unknown - Attention is not Explanation.pdf:pdf},
isbn = {1902.10186v3},
title = {{Attention is not Explanation}},
url = {https://github.com/successar/}
}
@article{Lin2017,
abstract = {This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.},
archivePrefix = {arXiv},
arxivId = {1703.03130},
author = {Lin, Zhouhan and Feng, Minwei and dos Santos, Cicero Nogueira and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
eprint = {1703.03130},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Lin et al. - 2017 - A Structured Self-attentive Sentence Embedding.pdf:pdf},
month = {mar},
title = {{A Structured Self-attentive Sentence Embedding}},
url = {http://arxiv.org/abs/1703.03130},
year = {2017}
}
@inproceedings{Garland2019,
author = {Garland, Ryan and Goldberg, Sophia and {Mathiesen (Streetbees)}, Erik},
booktitle = {EurNLP},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Garland, Goldberg, Mathiesen (Streetbees) - 2019 - Embedding Knowledge Graphs as Languages with BERT.pdf:pdf},
title = {{Embedding Knowledge Graphs as Languages with BERT}},
year = {2019}
}
@article{Kang2013,
author = {Kang, Ning and Singh, Bharat and Afzal, Zubair and van Mulligen, Erik M and Kors, Jan A},
doi = {10.1136/amiajnl-2012-001173},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Kang et al. - 2013 - Using rule-based natural language processing to improve disease normalization in biomedical text.pdf:pdf},
issn = {1067-5027},
journal = {Journal of the American Medical Informatics Association},
month = {sep},
number = {5},
pages = {876--881},
publisher = {Narnia},
title = {{Using rule-based natural language processing to improve disease normalization in biomedical text}},
url = {https://academic.oup.com/jamia/article-lookup/doi/10.1136/amiajnl-2012-001173},
volume = {20},
year = {2013}
}
@techreport{Caruana1997,
abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
author = {Caruana, Rich},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/{\pounds}    {\S}    - 1997 - Multitask Learning.pdf:pdf},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel regression,multitask,multitask learning,parallel transfer,supervised learning},
mendeley-tags = {multitask},
pages = {41--75},
publisher = {Kluwer Academic Publishers},
title = {{Multitask Learning *}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.8707{\&}rep=rep1{\&}type=pdf},
volume = {28},
year = {1997}
}
@techreport{Sun2019,
abstract = {Aspect-based sentiment analysis (ABSA), which aims to identify fine-grained opinion polarity towards a specific aspect, is a challenging subtask of sentiment analysis (SA). In this paper, we construct an auxiliary sentence from the aspect and convert ABSA to a sentence-pair classification task, such as question answering (QA) and natural language inference (NLI). We fine-tune the pre-trained model from BERT and achieve new state-of-the-art results on SentiHood and SemEval-2014 Task 4 datasets. 1},
archivePrefix = {arXiv},
arxivId = {1903.09588v1},
author = {Sun, Chi and Huang, Luyao and Qiu, Xipeng},
eprint = {1903.09588v1},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Sun, Huang, Qiu - 2019 - Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence.pdf:pdf},
title = {{Utilizing BERT for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence}},
year = {2019}
}
@article{Dai2019,
abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
archivePrefix = {arXiv},
arxivId = {1901.02860},
author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
eprint = {1901.02860},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a Fixed-Length Context.pdf:pdf},
month = {jan},
title = {{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}},
url = {http://arxiv.org/abs/1901.02860},
year = {2019}
}
@techreport{Gu,
abstract = {In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem, and we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen Euro-pean languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro, Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outper-forms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT'16 by seeing only 16,000 translated words ( 600 parallel sentences).},
author = {Gu, Jiatao and Wang, Yong and Chen, Yun and Cho, Kyunghyun and Li, Victor OK},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Gu et al. - Unknown - Meta-Learning for Low-Resource Neural Machine Translation.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
pages = {3622--3631},
title = {{Meta-Learning for Low-Resource Neural Machine Translation}},
url = {https://aclweb.org/anthology/D18-1398}
}
@article{Wang2018,
abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
archivePrefix = {arXiv},
arxivId = {1804.07461},
author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
eprint = {1804.07461},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Wang et al. - 2018 - GLUE A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.pdf:pdf},
month = {apr},
title = {{GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}},
url = {http://arxiv.org/abs/1804.07461},
year = {2018}
}
@techreport{Vaswani,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
archivePrefix = {arXiv},
arxivId = {1706.03762v5},
author = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
eprint = {1706.03762v5},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Vaswani et al. - Unknown - Attention Is All You Need.pdf:pdf},
title = {{Attention Is All You Need}},
url = {https://arxiv.org/pdf/1706.03762.pdf}
}
@techreport{Collobert,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398v1},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
eprint = {1103.0398v1},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Collobert et al. - Unknown - Natural Language Processing (almost) from Scratch.0398v1:0398v1},
keywords = {Natural Language Processing,Neural Networks},
title = {{Natural Language Processing (almost) from Scratch}},
url = {https://arxiv.org/pdf/1103.0398.pdf}
}
@techreport{Elman1990,
author = {Elman, Jeffreyl},
booktitle = {COGNITIVE SCIENCE},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Elman - 1990 - Finding Structure in Time.pdf:pdf},
pages = {179--211},
title = {{Finding Structure in Time}},
url = {https://pdfs.semanticscholar.org/b71e/c700edfec2b969c9d27d33eac09188290294.pdf},
volume = {14},
year = {1990}
}
@techreport{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference-sometimes prohibitively so in the case of very large data sets and large models. Several authors have also charged that NMT systems lack robustness, particularly when input sentences contain rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. To directly optimize the translation BLEU scores, we consider refining the models by using reinforcement learning, but we found that the improvement in the BLEU scores did not reflect in the human evaluation. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
annote = {wordpieces reference},
archivePrefix = {arXiv},
arxivId = {1609.08144v2},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144v2},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Wu et al. - Unknown - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {https://arxiv.org/pdf/1609.08144.pdf},
year = {2016}
}
@techreport{Yu,
abstract = {We study few-shot learning in natural language domains. Compared to many existing works that apply either metric-based or optimization-based meta-learning to image domain with low inter-task variance, we consider a more realistic setting, where tasks are diverse. However, it imposes tremendous difficulties to existing state-of-the-art metric-based algorithms since a single metric is insufficient to capture complex task variations in natural language domain. To alleviate the problem , we propose an adaptive metric learning approach that automatically determines the best weighted combination from a set of met-rics obtained from meta-training tasks for a newly seen few-shot task. Extensive quantitative evaluations on real-world sentiment analysis and dialog intent classification datasets demonstrate that the proposed method performs favorably against state-of-the-art few shot learning algorithms in terms of predictive accuracy. We make our code and data available for further study. 1},
author = {Yu, Mo and Guo, Xiaoxiao and Yi, Jinfeng and {Chang Saloni Potdar Yu Cheng Gerald Tesauro Haoyu Wang Bowen Zhou}, Shiyu},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Yu et al. - Unknown - Diverse Few-Shot Text Classification with Multiple Metrics.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
pages = {1206--1215},
title = {{Diverse Few-Shot Text Classification with Multiple Metrics}},
url = {https://github.com/Gorov/DiverseFewShot{\_}Amazon}
}
@article{Liu2019,
abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
archivePrefix = {arXiv},
arxivId = {1907.11692},
author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
eprint = {1907.11692},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining Approach.pdf:pdf},
month = {jul},
title = {{RoBERTa: A Robustly Optimized BERT Pretraining Approach}},
url = {http://arxiv.org/abs/1907.11692},
year = {2019}
}
@techreport{Chiticariu2013,
abstract = {The rise of "Big Data" analytics over unstruc-tured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia: while rule-based IE dominates the commercial world, it is widely regarded as dead-end technology by the academia. We believe the disconnect stems from the way in which the two communities measure the benefits and costs of IE, as well as academia's perception that rule-based IE is devoid of research challenges. We make a case for the importance of rule-based IE to industry practitioners. We then lay out a research agenda in advancing the state-of-the-art in rule-based IE systems which we believe has the potential to bridge the gap between academic research and industry practice.},
author = {Chiticariu, Laura and Li, Yunyao and Reiss, Frederick R},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Chiticariu, Li, Reiss - 2013 - Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems!.pdf:pdf},
pages = {18--21},
title = {{Rule-based Information Extraction is Dead! Long Live Rule-based Information Extraction Systems!}},
url = {https://www.aclweb.org/anthology/D13-1079},
year = {2013}
}
@article{Finn2017,
abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
archivePrefix = {arXiv},
arxivId = {1703.03400},
author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
eprint = {1703.03400},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Finn, Abbeel, Levine - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.pdf:pdf},
keywords = {meta},
month = {mar},
title = {{Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}},
url = {http://arxiv.org/abs/1703.03400},
year = {2017}
}
@article{Tenney2019,
abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
archivePrefix = {arXiv},
arxivId = {1905.05950},
author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.05950},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers//Tenney, Das, Pavlick - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf:pdf},
month = {may},
title = {{BERT Rediscovers the Classical NLP Pipeline}},
url = {http://arxiv.org/abs/1905.05950},
year = {2019}
}
@techreport{Brill,
abstract = {Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rule-based tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below.},
author = {Brill, Eric},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Brill - Unknown - A SIMPLE RULE-BASED PART OF SPEECH TAGGER.pdf:pdf},
title = {{A SIMPLE RULE-BASED PART OF SPEECH TAGGER}},
url = {https://apps.dtic.mil/dtic/tr/fulltext/u2/a460532.pdf}
}
@phdthesis{RuderThesis,
author = {Ruder, Sebastian},
keywords = {multitask},
mendeley-tags = {multitask},
school = {National University of Ireland},
title = {{Neural Transfer Learning for Natural Language Processing}},
year = {2019}
}
@article{Tenney2019a,
abstract = {Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.},
annote = {Reference for NLP pipeline},
archivePrefix = {arXiv},
arxivId = {1905.06316},
author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R Thomas and Kim, Najoung and {Van Durme}, Benjamin and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
eprint = {1905.06316},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Tenney et al. - 2019 - What do you learn from context Probing for sentence structure in contextualized word representations.pdf:pdf},
month = {may},
title = {{What do you learn from context? Probing for sentence structure in contextualized word representations}},
url = {http://arxiv.org/abs/1905.06316},
year = {2019}
}
@article{Kullback1951,
author = {Kullback, S. and Leibler, R. A.},
doi = {10.1214/aoms/1177729694},
issn = {0003-4851},
journal = {The Annals of Mathematical Statistics},
month = {mar},
number = {1},
pages = {79--86},
publisher = {Institute of Mathematical Statistics},
title = {{On Information and Sufficiency}},
url = {http://projecteuclid.org/euclid.aoms/1177729694},
volume = {22},
year = {1951}
}
@article{Taylor1953,
author = {Taylor, Wilson L.},
doi = {10.1177/107769905303000401},
issn = {0197-2448},
journal = {Journalism Bulletin},
month = {sep},
number = {4},
pages = {415--433},
title = {{Cloze Procedure: A New Tool for Measuring Readability}},
url = {http://journals.sagepub.com/doi/10.1177/107769905303000401},
volume = {30},
year = {1953}
}
@book{Graves2013,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connec-tionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cur-sive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
annote = {Deep Bidirectional LSTMs},
author = {Graves, Alex and Mohamed, Abdel-Rahman and Hinton, Geoffrey},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Graves, Mohamed, Hinton - Unknown - SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS.pdf:pdf},
isbn = {9781479903566},
title = {{SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS}},
url = {https://wiki.inf.ed.ac.uk/twiki/pub/CSTR/ListenSemester2201213/graves-icassp2013.pdf},
year = {2013}
}
@techreport{Kuleshov2000,
abstract = {The stochastic multi-armed bandit problem is an important model for studying the exploration-exploitation tradeoff in reinforcement learning. Although many algorithms for the problem are well-understood theoretically, empirical confirmation of their effectiveness is generally scarce. This paper presents a thorough empirical study of the most popular multi-armed bandit algorithms. Three important observations can be made from our results. Firstly, simple heuristics such as-greedy and Boltzmann exploration outperform theoretically sound algorithms on most settings by a significant margin. Secondly, the performance of most algorithms varies dramatically with the parameters of the bandit problem. Our study identifies for each algorithm the settings where it performs well, and the settings where it performs poorly. These properties are not described by current theory, even though they can be exploited in practice in the design of heuristics. Thirdly, the algorithms' performance relative each to other is affected only by the number of bandit arms and the variance of the rewards. This finding may guide the design of subsequent empirical evaluations. In the second part of the paper, we turn our attention to an important area of application of bandit algorithms: clinical trials. Although the design of clinical trials has been one of the principal practical problems motivating research on multi-armed bandits, bandit algorithms have never been evaluated as potential treatment allocation strategies. Using data from a real study, we simulate the outcome that a 2001-2002 clinical trial would have had if bandit algorithms had been used to allocate patients to treatments. We find that an adaptive trial would have successfully treated at least 50{\%} more patients, while significantly reducing the number of adverse effects and increasing patient retention. At the end of the trial, the best treatment could have still been identified with a high level of statistical confidence. Our findings demonstrate that bandit algorithms are attractive alternatives to current adaptive treatment allocation strategies.},
archivePrefix = {arXiv},
arxivId = {1402.6028v1},
author = {Kuleshov, Volodymyr and Precup, Doina},
booktitle = {Journal of Machine Learning Research},
eprint = {1402.6028v1},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Kuleshov, Precup - 2000 - Algorithms for the multi-armed bandit problem.pdf:pdf},
pages = {1--48},
title = {{Algorithms for the multi-armed bandit problem}},
url = {https://arxiv.org/pdf/1402.6028.pdf},
volume = {1},
year = {2000}
}
@techreport{Nichol,
abstract = {This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.},
archivePrefix = {arXiv},
arxivId = {1803.02999v3},
author = {Nichol, Alex and Achiam, Joshua and Openai, John Schulman},
eprint = {1803.02999v3},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Nichol, Achiam, Openai - Unknown - On First-Order Meta-Learning Algorithms.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
title = {{On First-Order Meta-Learning Algorithms}},
url = {https://arxiv.org/pdf/1803.02999.pdf}
}
@techreport{Kiritchenko2014,
abstract = {Reviews depict sentiments of customers towards various aspects of a product or service. Some of these aspects can be grouped into coarser aspect categories. SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper , we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.},
author = {Kiritchenko, Svetlana and Zhu, Xiaodan and Cherry, Colin and Mohammad, Saif M},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Kiritchenko et al. - 2014 - NRC-Canada-2014 Detecting Aspects and Sentiment in Customer Reviews.pdf:pdf},
pages = {437--442},
title = {{NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews}},
url = {http://nlp.stanford.edu/software/corenlp.shtml},
year = {2014}
}
@article{Paulus2017,
abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We introduce a neural network model with a novel intra-attention that attends over the input and continuously generated output separately, and a new training method that combines standard supervised word prediction and reinforcement learning (RL). Models trained only with supervised learning often exhibit "exposure bias" - they assume ground truth is provided at each step during training. However, when standard word prediction is combined with the global sequence prediction training of RL the resulting summaries become more readable. We evaluate this model on the CNN/Daily Mail and New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the CNN/Daily Mail dataset, an improvement over previous state-of-the-art models. Human evaluation also shows that our model produces higher quality summaries.},
archivePrefix = {arXiv},
arxivId = {1705.04304},
author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
eprint = {1705.04304},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Paulus, Xiong, Socher - 2017 - A Deep Reinforced Model for Abstractive Summarization.pdf:pdf},
month = {may},
title = {{A Deep Reinforced Model for Abstractive Summarization}},
url = {http://arxiv.org/abs/1705.04304},
year = {2017}
}
@article{Stickland2019,
abstract = {Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a single BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or `projected attention layers', we match the performance of separately fine-tuned models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.},
archivePrefix = {arXiv},
arxivId = {1902.02671},
author = {Stickland, Asa Cooper and Murray, Iain},
eprint = {1902.02671},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Stickland, Murray - 2019 - BERT and PALs Projected Attention Layers for Efficient Adaptation in Multi-Task Learning.pdf:pdf},
keywords = {multitask},
month = {feb},
title = {{BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning}},
url = {http://arxiv.org/abs/1902.02671},
year = {2019}
}
@misc{tensorboard,
author = {Google and Lanpa},
title = {{Tensorboard {\&} TensorboardX}},
url = {https://github.com/lanpa/tensorboardX},
year = {2019}
}
@techreport{COCO,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312v3},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C Lawrence and Dol{\'{i}}, Piotr},
eprint = {1405.0312v3},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Lin et al. - Unknown - Microsoft COCO Common Objects in Context.pdf:pdf},
title = {{Microsoft COCO: Common Objects in Context}},
url = {https://arxiv.org/pdf/1405.0312.pdf}
}
@inproceedings{ImageNet,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206848},
isbn = {978-1-4244-3992-8},
month = {jun},
pages = {248--255},
publisher = {IEEE},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {https://ieeexplore.ieee.org/document/5206848/},
year = {2009}
}
@techreport{Zhang2019,
abstract = {Text classification tends to be difficult when the data is deficient or when it is required to adapt to unseen classes. In such challenging scenarios, recent studies have often used meta-learning to simulate the few-shot task, thus negating explicit common linguistic features across tasks. Deep language representations have proven to be very effective forms of unsupervised pretraining, yielding contextual-ized features that capture linguistic properties and benefit downstream natural language understanding tasks. However, the effect of pretrained language representation for few-shot learning on text classification tasks is still not well understood. In this study, we design a few-shot learning model with pretrained language representations and report the empirical results. We show that our approach is not only simple but also produces state-of-the-art performance on a well-studied sentiment classification dataset. It can thus be further suggested that pretraining could be a promising solution for few shot learning of many other NLP tasks. The code and the dataset to replicate the experiments are made available at https://github.com/zxlzr/FewShotNLP.},
archivePrefix = {arXiv},
arxivId = {1908.08788v1},
author = {Zhang, Ningyu and Sun, Zhanlin and Deng, Shumin and Chen, Jiaoyan and Chen, Huajun},
eprint = {1908.08788v1},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Zhang et al. - 2019 - Improving Few-shot Text Classification via Pretrained Language Representations.pdf:pdf},
keywords = {meta},
mendeley-tags = {meta},
title = {{Improving Few-shot Text Classification via Pretrained Language Representations}},
url = {https://github.com/Gorov/DiverseFewShot},
year = {2019}
}
