Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Parisi2018,
abstract = {Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.},
archivePrefix = {arXiv},
arxivId = {1802.07569},
author = {Parisi, German I. and Kemker, Ronald and Part, Jose L. and Kanan, Christopher and Wermter, Stefan},
doi = {10.1016/j.neunet.2019.01.012.},
eprint = {1802.07569},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Parisi et al. - 2018 - Continual Lifelong Learning with Neural Networks A Review.pdf:pdf},
keywords = {multitask},
mendeley-tags = {multitask},
month = {feb},
title = {{Continual Lifelong Learning with Neural Networks: A Review}},
url = {http://arxiv.org/abs/1802.07569 http://dx.doi.org/10.1016/j.neunet.2019.01.012.},
year = {2018}
}
@article{Ruder2017,
abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
archivePrefix = {arXiv},
arxivId = {1706.05098},
author = {Ruder, Sebastian},
eprint = {1706.05098},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural Networks.pdf:pdf},
keywords = {multitask},
month = {jun},
title = {{An Overview of Multi-Task Learning in Deep Neural Networks}},
url = {http://arxiv.org/abs/1706.05098},
year = {2017}
}
@article{Sun2019a,
abstract = {Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.},
archivePrefix = {arXiv},
arxivId = {1907.12412},
author = {Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and Tian, Hao and Wu, Hua and Wang, Haifeng},
eprint = {1907.12412},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Sun et al. - 2019 - ERNIE 2.0 A Continual Pre-training Framework for Language Understanding.pdf:pdf},
keywords = {multitask},
mendeley-tags = {multitask},
month = {jul},
title = {{ERNIE 2.0: A Continual Pre-training Framework for Language Understanding}},
url = {http://arxiv.org/abs/1907.12412},
year = {2019}
}
@article{Sanh2018,
abstract = {Much effort has been devoted to evaluate whether multi-task learning can be leveraged to learn rich representations that can be used in various Natural Language Processing (NLP) down-stream applications. However, there is still a lack of understanding of the settings in which multi-task learning has a significant effect. In this work, we introduce a hierarchical model trained in a multi-task learning setup on a set of carefully selected semantic tasks. The model is trained in a hierarchical fashion to introduce an inductive bias by supervising a set of low level tasks at the bottom layers of the model and more complex tasks at the top layers of the model. This model achieves state-of-the-art results on a number of tasks, namely Named Entity Recognition, Entity Mention Detection and Relation Extraction without hand-engineered features or external NLP tools like syntactic parsers. The hierarchical training supervision induces a set of shared semantic representations at lower layers of the model. We show that as we move from the bottom to the top layers of the model, the hidden states of the layers tend to represent more complex semantic information.},
annote = {https://medium.com/dair-ai/hmtl-multi-task-learning-for-state-of-the-art-nlp-245572bbb601},
archivePrefix = {arXiv},
arxivId = {1811.06031},
author = {Sanh, Victor and Wolf, Thomas and Ruder, Sebastian},
eprint = {1811.06031},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Sanh, Wolf, Ruder - 2018 - A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks.pdf:pdf},
keywords = {multitask},
mendeley-tags = {multitask},
month = {nov},
title = {{A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks}},
url = {http://arxiv.org/abs/1811.06031},
year = {2018}
}
@article{Baxter1997,
author = {Baxter, Jonathan},
doi = {10.1023/A:1007327622663},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Baxter - 1997 - A BayesianInformation Theoretic Model of Learning to Learn via Multiple Task Sampling.pdf:pdf},
issn = {08856125},
journal = {Machine Learning},
keywords = {multitask},
number = {1},
pages = {7--39},
publisher = {Kluwer Academic Publishers},
title = {{A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling}},
url = {http://link.springer.com/10.1023/A:1007327622663},
volume = {28},
year = {1997}
}
@techreport{Caruana1997,
abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
author = {Caruana, Rich},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/{\pounds} ¢ ¥ ¤ {\S} ¦ ¨ ¤ - 1997 - Multitask Learning.pdf:pdf},
keywords = {backpropagation,generalization,inductive transfer,k-nearest neighbor,kernel regression,multitask,multitask learning,parallel transfer,supervised learning},
mendeley-tags = {multitask},
pages = {41--75},
publisher = {Kluwer Academic Publishers},
title = {{Multitask Learning *}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.121.8707{\&}rep=rep1{\&}type=pdf},
volume = {28},
year = {1997}
}
@phdthesis{RuderThesis,
author = {Ruder, Sebastian},
keywords = {multitask},
mendeley-tags = {multitask},
school = {National University of Ireland},
title = {{Neural Transfer Learning for Natural Language Processing}},
year = {2019}
}
@article{Stickland2019,
abstract = {Multi-task learning shares information between related tasks, sometimes reducing the number of parameters required. State-of-the-art results across multiple natural language understanding tasks in the GLUE benchmark have previously used transfer from a single large task: unsupervised pre-training with BERT, where a separate BERT model was fine-tuned for each task. We explore multi-task approaches that share a single BERT model with a small number of additional task-specific parameters. Using new adaptation modules, PALs or `projected attention layers', we match the performance of separately fine-tuned models on the GLUE benchmark with roughly 7 times fewer parameters, and obtain state-of-the-art results on the Recognizing Textual Entailment dataset.},
archivePrefix = {arXiv},
arxivId = {1902.02671},
author = {Stickland, Asa Cooper and Murray, Iain},
eprint = {1902.02671},
file = {:Users/ryanjenkinson/Documents/MSc Project/LaTeX Project/papers/Stickland, Murray - 2019 - BERT and PALs Projected Attention Layers for Efficient Adaptation in Multi-Task Learning.pdf:pdf},
keywords = {multitask},
month = {feb},
title = {{BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning}},
url = {http://arxiv.org/abs/1902.02671},
year = {2019}
}
