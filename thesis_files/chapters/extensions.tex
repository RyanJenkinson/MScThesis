% ------------------------------------------------
% Nomenclature list - update if needed
% \nomenclature[G]{$\mathcal{D}$}{A dataset}

% ------------------------------------------------
\chapter{Extensions} \label{chapter:extensions}

In this section we propose extensions to our current study, in the form of methodological refinements, which would be quick and easy variants to the investigation we have described, then looking at methodological extensions. We also fully outline our Meta Learning strategy for Few Shot Text Classification, since we were unable to conduct any meaningful experiments due to time constraints.

\section{Methodological Refinements}

\subsection{Training Time} \label{section:extensions:numstepsperepoch}
The number of steps taken per epoch is somewhat of an arbritrary definition, indeed as is the notion of the number of epochs on which to train for. All that matters is the number of gradient descent steps we take, and ensuring that we cycle through our various tasks in a way that reasonably sees ``a good quantity" of examples from each task. We chose to do this with a sampling schema and set the number of epochs to match those used in the ABSA paper on which we intended to build upon \cite{Sun2019}. As per this paper, the metrics were reported at the end of these 4 epochs, rather than taking the best metrics achieved at some point during the 4 epochs. This means that if we configured an early stopping routine more optimally, we would have almost certainly achieved better results. The reason we chose not to was so that we could compare all of our models to the baselines in the papers, but this methodology has the unfortunate possibility of overfitting in certain cases. In the multitask setting, however, the additional tasks act as effective regularisers; introducing inductive biases into the model require the model to learn representations that satisfy all task hypotheses rather than just the single primary task. In our experimentation and inspection of the loss curves plotted, the models did not drastically overfit, and so this effect is negligible.

There is much research to be done in terms of reasonably understanding when a model should switch between subtasks and at what time in order to gain new knowledge from a different situation. Early Stopping protocols ensure the model does not overfit, but its often the case that base language models are actually underfitting, such as the case in BERT \cite{Devlin2018}.

\subsection{Subtasks} \label{section:extensions:subtasks}
Of course, the tasks we chose were motivated by the preimposed task hierarchy, but this was somewhat arbritrary - motivated only by human intuition and popular datasets that solve these tasks. We could have additionally added Question Answering (QA) and Natural Language Inference (NLI) tasks, that would almost certainly have improved the performance given the (T)ABSA sentence construction method defined in Section \ref{section:background:tabsasentenceconstruction}. However, we chose not to do this since we wanted to investigate the importance of the NLI pretraining objective (that was used in BERT, but not XLNet) on performance. Additionally, the implementational details for QA tasks are somewhat more complex, and would have been a timely procedure to implement from scratch and augment our consistent data setup with due to the inclusion of start and end span tokens in particular, but additionally adding these tasks is clearly an extension and left for the interested reader.

Another interesting experimental extension would have been to consider ``unrelated" subtasks in this context as a baseline/control case on which to judge experimental results. We felt this wasnt necessary since the single task performance metric provided a reasonable baseline from which we compared all subsequent experimental results, but it would have been interesting nonetheless and would only show to strengthen our hypothesis if the results performed worse than our selected supporting tasks for ABSA. We could hypothesise that generally multitask learning alone helps, and different latent representations produce useful downstream abilities, and the sampling schemas effect is negligible. An interesting hypothesis on which to test is how performance scales with the number of supporting tasks. On the one hand, one would expect diminishing returns as the number of tasks scale, since the optimal manifold on which we want to learn our downstream representations is shared by many of these tasks, introducing a redundency in terms of the input tasks, but it could be the case that we are still slightly underfitting and these additional tasks gradient descent the model into various regions of the parameter space on which better representations can be learned.

\subsection{Task Distributions} \label{section:extensions:taskweightings}
An interesting extensions would be if the task distribution were itself learned in some way, e.g using reinforcement learning techniques. The setup in this paper was somewhat of an arbritrary choice but provides a proof of concept for investigation. Ifthe distribution was itself learnable, one could see if the model learns to prioritise tasks in a similar way to the dsitributions we defined. However, it is possible it would not learn a meaningful distribution and just collapse onto a random structure as this is ``good enough" i.e. the model doesn't recieve a strong enough signal from the task distribution as to its relevancy, and there may be an element of hyperparameter tuning involved.

A proposal is a Reinforcement Learning type approach whereby a sampling schedule reflects how the loss decreases for each task - if it decreases the loss on the main task quite quickly then we place more emphasis on it and vice versa. This is a type of Multi Armed Bandit style setup \cite{Kuleshov2000}, where the action space is deciding which task to select a minibatch from for the next step in training.

\subsection{Hyperparameter Tuning} \label{section:extensions:hyperparametertuning}
As noted in Section \ref{section:experiments:hyperparameters}, an exhaustive grid or random hyperparameter search would have proved costly given our computational constraints, and was thus omitted from the experiments. Additionally, it is clear that hyperparameter optimisation would improve the performance but, as is common in language modelling research, the space of hyperparameters is far too large to search, and the time complexity of running these operations means only those with access to large scale distributed computing software can reasonably conduct such analyses.

As well as the hyperparameters controlling the architecture of the model, we could also vary the hyperparameters of the optimisers, or indeed the way the model summarises a sentence input (the ``sequence summary token" described in Section \ref{section:methodology:modelsetup}) to instead average the token embeddings rather than using the special classification token embedding. All of this functionality exists in the library that was built, but could not be tested over due to the computational constraints this exponentially large search would entail.

\subsection{Multiple Runs}
In our experiments, we fixed a certain random seed in which we conducted all of our experiments. Different random seeds would correspond to different task samplings and stochastic effects in the optimiser. As such, for a more rigorous study, one would want to repeat the experiments several times across several random seeds and take an average. This would make the study much more robust, but was outside the computational constraints of this study.

\section{Methodological Extensions}
\subsection{Mutual Information}
We could utilise a different approach for ABSA that would prove a very interesting extension indeed. We could investigate the role of \textit{mutual information} by introducing a hyperparameter $\lambda$ that weights the forwards and backwards information from \texttt{text\_a} (the input sentence) and \texttt{text\_b} (in the ABSA case, the corresponding pseudosentence/question asked of the input sentence as per Section \ref{section:background:tabsasentenceconstruction}). Essentially, this is like using the question to predict the response AND using the response to predict the question, and one would expect performance increases to occur if using this methodology.

\subsection{Label Smoothing}
Since neutral sentiment is a very fuzzy subjective sentimental state, training samples which are labelled neutral are unreliable. As such, we should probably employ a label smoothing regularisation term in the loss function, which penalises low entopy output distributions \cite{Szegedy}. It can reduce overfitting by preventing the network from assigning the full probability to each example in training by replacing the hard 0/1 targets with smoothed values such as 0.1 and 0.9. This trick would almost certainly improve performance, since Table \ref{table:extensions:semevalclasses} shows the distribution of sentiment classes in the ABSA Dataset is somewhat disjointed.
\begin{center}
	\begin{tabular}{c@{\qquad}ccc@{\qquad}ccc}
		\toprule
		\multirow{2}{*}{\raisebox{-\heavyrulewidth}{Dataset}} & \multicolumn{2}{c}{Positive} & \multicolumn{2}{c}{Neutral} &  \multicolumn{2}{c}{Negative} \\
		\cmidrule{2-7}
		& Train & Test & Train & Test & Train & Test\\
		\midrule
		Restaurants & 2164 & 728 & 637 & 196 & 807 & 1961 \\
		\bottomrule
	\end{tabular}
	\captionof{table}{A breakdown of the sentiment classes in the SemEval ABSA task}
	\label{table:extensions:semevalclasses}
\end{center}

\section{Meta Learning} \label{section:extensions:meta}
In terms of the flow of this thesis, we have been able to effectively build systems that leverage alternative task representations to best solve the ABSA task. We described several methodologies to improve performance on this task for use cases in academia or commercially, but we want to extend this one step further.

Given the ABSA task is quite niche in terms of large datasets, and quite granular in terms of the number of examples corresponding to a certain category, and the category classes differ from domain to domain\footnote{Indeed in the SemEval 2014 task, where we focused on Restaurant datasets, there was another dataset released that focused on Laptop reviews, where the categories were e.g battery and screen, irrelevant in the context of restaurants}, we motivate a use case for the novel application of Meta Learning to Natural Language Processing systems. The outline of the most popular and recent meta learning algorithms can be seen in Section \ref{section:background:meta}. Importantly, these algorithms are model agnostic, meaning any function that is differentiable (including our Language Models) can be used in this meta learning paradigm.

We have to set up the problem appropriately for meta learning, and in doing so we must imagine a reasonable task that we are faced with at test time. One such motivation for this work in the corporate setting was that clients often come asking how people felt about a particular category: a hot topic and the source of this example will be the category ``sustainability". This category does not exist in the dataset literature, and thus we can only hope to fine tune our model on this category. However, additionally, there is not much labelled/annotated data on this category. It is challenging to label hundreds of thousands of examples, but relatively simple to label around 100 examples. If we could design a model that is able to learn from a few examples to best predict all new incoming data, then that would solve this problem. Academically, the problem of learning complicated sentimental nuance from few examples is an interesting research problem. The hope is that the meta learning setup improves generalisation ability; the model has learned to learn from a few examples, meaning it better generalises onto the new aspect category. 

Concurrently with this thesis, this idea of few shot text classification with pre-initialised language models was independently replicated by researchers at Alibaba specifically for the case of BERT \cite{Yu}. They work on an already prepared meta learning text dataset, whereas we provide utilities to transform any dataset into this meta learning framework in our library, but report better results than applying MAML on its own to the randomly initialised language model, thereby validating the usage of language model initialisation for a ``better" starting point for training.

In our work, we use Reptile \cite{Nichol} to give a feasable methodology for training in the few shot text classification regime: For each task, which for us now is defined by a unique emotion category for the commerical dataset, or a unique aspect in the SemEval dataset, we take $K$ positive examples of this task as well as $K$ negative examples of this task. We do this for every category. Then, we construct mini datasets considering of $2K$ examples for each of the $C$ categories (e.g. $C=34$ in the case of the Streetbees Data). At training time, we perform a series of meta training steps, which consist of passing each of these mini batches of category specific datasets through the model, observing and backpropagating the loss for each category and storing all of these weight updates on each of the inner steps, then taking an outer meta training gradient descent step as per Algorithm \ref{alg:background:reptile} in Section \ref{section:background:meta}. We found in our work that the gradient update rules were fragile and possibly mathematically unstable and thus the language models did not learn effectively. A more thorough investigation of learning rates and the code would have to be conducted to continue this extension, but we provide a baseline implementation that runs end-to-end in the \texttt{TraMML} library. For more information, see Appendix \ref{appendix:code}.

\section{Knowledge Graph Extensions}
The company is currently working on a NER tagger, which is able to tag products and brands. This could be used in conjunction with the TABSA style setup: for each sentence, once targets $t$ have been identified, we could prepare auxilliary questions asking about a fixed aspect set $\mathcal{A}$ about each of the targets and build a knowledge graph using the methodology outlined in this paper.