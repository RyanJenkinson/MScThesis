\BOOKMARK [0][-]{chapter*.2}{Tables and Figures}{}% 1
\BOOKMARK [1][-]{section*.3}{List of Tables}{chapter*.2}% 2
\BOOKMARK [1][-]{chapter*.4}{List of Figures}{chapter*.2}% 3
\BOOKMARK [0][-]{chapter*.6}{List of Common Terms and Symbols}{}% 4
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 5
\BOOKMARK [1][-]{section.1.1}{\(A brief\) History of NLP}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.2}{Thesis Focus}{chapter.1}% 7
\BOOKMARK [2][-]{subsection.1.2.1}{Aspect Based Sentiment Analysis}{section.1.2}% 8
\BOOKMARK [2][-]{subsection.1.2.2}{Transfer Learning}{section.1.2}% 9
\BOOKMARK [2][-]{subsection.1.2.3}{Multitask Learning}{section.1.2}% 10
\BOOKMARK [2][-]{subsection.1.2.4}{Meta Learning}{section.1.2}% 11
\BOOKMARK [1][-]{section.1.3}{Project Aims \046 Our Contributions}{chapter.1}% 12
\BOOKMARK [1][-]{section.1.4}{Commercial Applications}{chapter.1}% 13
\BOOKMARK [1][-]{section.1.5}{Public Code Repository}{chapter.1}% 14
\BOOKMARK [0][-]{chapter.2}{Background}{}% 15
\BOOKMARK [1][-]{section.2.1}{The Evolution of Language Modelling}{chapter.2}% 16
\BOOKMARK [2][-]{subsection.2.1.1}{Problem Setup}{section.2.1}% 17
\BOOKMARK [2][-]{subsection.2.1.2}{n-gram models}{section.2.1}% 18
\BOOKMARK [2][-]{subsection.2.1.3}{Pretrained Word Embeddings: Humble Beginnings}{section.2.1}% 19
\BOOKMARK [2][-]{subsection.2.1.4}{The Rise of Deep Learning: Using Sequentiality for Context}{section.2.1}% 20
\BOOKMARK [1][-]{section.2.2}{Current SOTA Language Modelling: Transformers}{chapter.2}% 21
\BOOKMARK [2][-]{subsection.2.2.1}{Transformer Architecture}{section.2.2}% 22
\BOOKMARK [2][-]{subsection.2.2.2}{BERT}{section.2.2}% 23
\BOOKMARK [2][-]{subsection.2.2.3}{XLNet}{section.2.2}% 24
\BOOKMARK [2][-]{subsection.2.2.4}{Casing in Language Models}{section.2.2}% 25
\BOOKMARK [1][-]{section.2.3}{Sentiment Analysis}{chapter.2}% 26
\BOOKMARK [2][-]{subsection.2.3.1}{SST-2}{section.2.3}% 27
\BOOKMARK [2][-]{subsection.2.3.2}{SemEval 2014 - ABSA}{section.2.3}% 28
\BOOKMARK [2][-]{subsection.2.3.3}{Sentihood - TABSA}{section.2.3}% 29
\BOOKMARK [2][-]{subsection.2.3.4}{\(T\)ABSA and LMs: Auxilliary Sentence Construction Method}{section.2.3}% 30
\BOOKMARK [2][-]{subsection.2.3.5}{Analysis of the TABSA Explosion Method}{section.2.3}% 31
\BOOKMARK [1][-]{section.2.4}{Multitask learning}{chapter.2}% 32
\BOOKMARK [2][-]{subsection.2.4.1}{Types of Multitask Learning}{section.2.4}% 33
\BOOKMARK [2][-]{subsection.2.4.2}{Sampling Tasks}{section.2.4}% 34
\BOOKMARK [2][-]{subsection.2.4.3}{Multitask learning applied to language modelling}{section.2.4}% 35
\BOOKMARK [2][-]{subsection.2.4.4}{Multitask learning applied to ABSA}{section.2.4}% 36
\BOOKMARK [2][-]{subsection.2.4.5}{Multitask learning for Knowledge Graph Construction}{section.2.4}% 37
\BOOKMARK [1][-]{section.2.5}{Meta Learning}{chapter.2}% 38
\BOOKMARK [2][-]{subsection.2.5.1}{How we learn to learn}{section.2.5}% 39
\BOOKMARK [2][-]{subsection.2.5.2}{MAML}{section.2.5}% 40
\BOOKMARK [2][-]{subsection.2.5.3}{Reptile \046 FOMAML}{section.2.5}% 41
\BOOKMARK [2][-]{subsection.2.5.4}{Meta Learning applied to Language Modelling}{section.2.5}% 42
\BOOKMARK [0][-]{chapter.3}{Methodology}{}% 43
\BOOKMARK [1][-]{section.3.1}{Hypothesis}{chapter.3}% 44
\BOOKMARK [1][-]{section.3.2}{Task Setup}{chapter.3}% 45
\BOOKMARK [2][-]{subsection.3.2.1}{Task Structure}{section.3.2}% 46
\BOOKMARK [2][-]{subsection.3.2.2}{Datasets}{section.3.2}% 47
\BOOKMARK [2][-]{subsection.3.2.3}{Task Distributions}{section.3.2}% 48
\BOOKMARK [1][-]{section.3.3}{Model Architecture Setup}{chapter.3}% 49
\BOOKMARK [1][-]{section.3.4}{Reporting}{chapter.3}% 50
\BOOKMARK [0][-]{chapter.4}{Experiments \046 Results}{}% 51
\BOOKMARK [1][-]{section.4.1}{Base Language Model Choices}{chapter.4}% 52
\BOOKMARK [1][-]{section.4.2}{SemEval \(ABSA\) Results}{chapter.4}% 53
\BOOKMARK [2][-]{subsection.4.2.1}{NLI Pretraining Task Importance}{section.4.2}% 54
\BOOKMARK [2][-]{subsection.4.2.2}{Analysis of Results}{section.4.2}% 55
\BOOKMARK [1][-]{section.4.3}{An Overview of SOTA Results Achieved}{chapter.4}% 56
\BOOKMARK [1][-]{section.4.4}{Streetbees Results}{chapter.4}% 57
\BOOKMARK [2][-]{subsection.4.4.1}{Multilabel Emotion Results}{section.4.4}% 58
\BOOKMARK [2][-]{subsection.4.4.2}{Generalisation Capabilities}{section.4.4}% 59
\BOOKMARK [1][-]{section.4.5}{Preliminary Experiments}{chapter.4}% 60
\BOOKMARK [2][-]{subsection.4.5.1}{Ensuring Baseline Performance}{section.4.5}% 61
\BOOKMARK [2][-]{subsection.4.5.2}{Gradual Unfreezing of Base Language Model Layers}{section.4.5}% 62
\BOOKMARK [2][-]{subsection.4.5.3}{Multitask learning for Sentiment Classification}{section.4.5}% 63
\BOOKMARK [1][-]{section.4.6}{Success of the Sampling Schemas}{chapter.4}% 64
\BOOKMARK [1][-]{section.4.7}{Remark on Task Distributions for Experimental Optimisation}{chapter.4}% 65
\BOOKMARK [1][-]{section.4.8}{Training Choices}{chapter.4}% 66
\BOOKMARK [2][-]{subsection.4.8.1}{Hyperparameters}{section.4.8}% 67
\BOOKMARK [2][-]{subsection.4.8.2}{Computing Resources}{section.4.8}% 68
\BOOKMARK [0][-]{chapter.5}{Extensions}{}% 69
\BOOKMARK [1][-]{section.5.1}{Methodological Refinements}{chapter.5}% 70
\BOOKMARK [2][-]{subsection.5.1.1}{Training Time}{section.5.1}% 71
\BOOKMARK [2][-]{subsection.5.1.2}{Subtasks}{section.5.1}% 72
\BOOKMARK [2][-]{subsection.5.1.3}{Task Distributions}{section.5.1}% 73
\BOOKMARK [2][-]{subsection.5.1.4}{Hyperparameter Tuning}{section.5.1}% 74
\BOOKMARK [2][-]{subsection.5.1.5}{Multiple Runs}{section.5.1}% 75
\BOOKMARK [1][-]{section.5.2}{Methodological Extensions}{chapter.5}% 76
\BOOKMARK [2][-]{subsection.5.2.1}{Mutual Information}{section.5.2}% 77
\BOOKMARK [2][-]{subsection.5.2.2}{Label Smoothing}{section.5.2}% 78
\BOOKMARK [1][-]{section.5.3}{Meta Learning}{chapter.5}% 79
\BOOKMARK [1][-]{section.5.4}{Knowledge Graph Extensions}{chapter.5}% 80
\BOOKMARK [0][-]{chapter.6}{Conclusion}{}% 81
\BOOKMARK [0][-]{appendix.A}{Streetbees: A Company Profile \046 Project Relevancy}{}% 82
\BOOKMARK [0][-]{appendix.B}{Additional Findings}{}% 83
\BOOKMARK [0][-]{appendix.C}{Code listings}{}% 84
\BOOKMARK [0][-]{table.C.1}{References}{}% 85
